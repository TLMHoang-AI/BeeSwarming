{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "import os\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(\"mfcc_features_buzz2/Train_mfcc_features.csv\")\n",
    "test_df = pd.read_csv(\"mfcc_features_buzz2/Test_mfcc_features.csv\")\n",
    "val_df = pd.read_csv(\"mfcc_features_buzz2/Val_mfcc_features.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = train_df.drop(columns=['file_name'])\n",
    "test_df = test_df.drop(columns=['file_name'])\n",
    "val_df = val_df.drop(columns=['file_name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = train_df.drop(columns=['label']), train_df['label']\n",
    "X_test, y_test = test_df.drop(columns=['label']), test_df['label']\n",
    "X_val, y_val = val_df.drop(columns=['label']), val_df['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape: (19002, 180), Test shape: (11200, 180), Val shape: (4449, 180)\n"
     ]
    }
   ],
   "source": [
    "print(f\"Train shape: {X_train.shape}, Test shape: {X_test.shape}, Val shape: {X_val.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X_train_Scaled = scaler.fit_transform(X_train)\n",
    "X_test_Scaled = scaler.transform(X_test)\n",
    "X_val_Scaled = scaler.transform(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best n_neighbors: 7 with Cross-Validation Accuracy: 0.9989\n"
     ]
    }
   ],
   "source": [
    "param_grid = {\n",
    "    'n_neighbors': list(range(3, 100, 2))\n",
    "}\n",
    "\n",
    "knn = KNeighborsClassifier()\n",
    "\n",
    "grid_search = GridSearchCV(estimator=knn, param_grid=param_grid, cv=5, scoring='accuracy')\n",
    "\n",
    "grid_search.fit(X_train_Scaled, y_train)\n",
    "\n",
    "best_n_neighbors = grid_search.best_params_['n_neighbors']\n",
    "best_score = grid_search.best_score_\n",
    "\n",
    "print(f\"Best n_neighbors: {best_n_neighbors} with Cross-Validation Accuracy: {best_score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Set (Raw MFCC) Performance:\n",
      "   Accuracy: 0.9438\n",
      "   F1 Score: 0.9468\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.89      0.94      5600\n",
      "           1       0.90      1.00      0.95      5600\n",
      "\n",
      "    accuracy                           0.94     11200\n",
      "   macro avg       0.95      0.94      0.94     11200\n",
      "weighted avg       0.95      0.94      0.94     11200\n",
      "\n",
      "Validation Set (Raw MFCC) Performance:\n",
      "   Accuracy: 0.9735\n",
      "   F1 Score: 0.9744\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.95      0.97      2200\n",
      "           1       0.95      1.00      0.97      2249\n",
      "\n",
      "    accuracy                           0.97      4449\n",
      "   macro avg       0.97      0.97      0.97      4449\n",
      "weighted avg       0.97      0.97      0.97      4449\n",
      "\n"
     ]
    }
   ],
   "source": [
    "knn = KNeighborsClassifier(n_neighbors=7)\n",
    "knn.fit(X_train_Scaled, y_train)\n",
    "\n",
    "y_pred_test = knn.predict(X_test_Scaled)\n",
    "\n",
    "y_pred_val = knn.predict(X_val_Scaled)\n",
    "\n",
    "def evaluate_model(y_true, y_pred, dataset_name):\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    f1 = f1_score(y_true, y_pred)\n",
    "    report = classification_report(y_true, y_pred)\n",
    "\n",
    "    print(f\"{dataset_name} Performance:\")\n",
    "    print(f\"   Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"   F1 Score: {f1:.4f}\\n\")\n",
    "    print(\"Classification Report:\\n\", report)\n",
    "    \n",
    "    return {\"accuracy\": accuracy, \"f1_score\": f1, \"report\": report}\n",
    "\n",
    "test_results = evaluate_model(y_test, y_pred_test, \"Test Set (Raw MFCC)\")\n",
    "\n",
    "val_results = evaluate_model(y_val, y_pred_val, \"Validation Set (Raw MFCC)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best C found: 0.01\n"
     ]
    }
   ],
   "source": [
    "param_grid = {'C': [0.001, 0.01, 0.1, 1, 10, 100]}\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_Val_Scaled = scaler.fit_transform(X_val)\n",
    "\n",
    "svm = SVC(kernel='linear')\n",
    "grid_search = GridSearchCV(svm, param_grid, cv=5, scoring='accuracy', n_jobs=-1)\n",
    "grid_search.fit(X_Val_Scaled, y_val)\n",
    "\n",
    "best_C = grid_search.best_params_['C']\n",
    "print(f\"Best C found: {best_C}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Set (Scaled MFCC) Performance:\n",
      "   Accuracy: 0.6466\n",
      "   F1 Score: 0.7389\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.29      0.45      5600\n",
      "           1       0.59      1.00      0.74      5600\n",
      "\n",
      "    accuracy                           0.65     11200\n",
      "   macro avg       0.79      0.65      0.60     11200\n",
      "weighted avg       0.79      0.65      0.60     11200\n",
      "\n",
      "Validation Set (Scaled MFCC) Performance:\n",
      "   Accuracy: 0.9903\n",
      "   F1 Score: 0.9905\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.98      0.99      2200\n",
      "           1       0.98      1.00      0.99      2249\n",
      "\n",
      "    accuracy                           0.99      4449\n",
      "   macro avg       0.99      0.99      0.99      4449\n",
      "weighted avg       0.99      0.99      0.99      4449\n",
      "\n"
     ]
    }
   ],
   "source": [
    "svm = SVC(kernel='linear', C=0.01, shrinking=False)\n",
    "svm.fit(X_train_Scaled, y_train)\n",
    "\n",
    "y_pred_test = svm.predict(X_test_Scaled)\n",
    "y_pred_val = svm.predict(X_val_Scaled)\n",
    "\n",
    "def evaluate_model(y_true, y_pred, dataset_name):\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    f1 = f1_score(y_true, y_pred)\n",
    "    report = classification_report(y_true, y_pred)\n",
    "\n",
    "    print(f\"{dataset_name} Performance:\")\n",
    "    print(f\"   Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"   F1 Score: {f1:.4f}\\n\")\n",
    "    print(\"Classification Report:\\n\", report)\n",
    "\n",
    "    return {\"accuracy\": accuracy, \"f1_score\": f1, \"report\": report}\n",
    "\n",
    "test_results = evaluate_model(y_test, y_pred_test, \"Test Set (Scaled MFCC)\")\n",
    "val_results = evaluate_model(y_val, y_pred_val, \"Validation Set (Scaled MFCC)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating Na√Øve Bayes on Raw MFCC Features:\n",
      "\n",
      "NB Raw (Test) Model Performance:\n",
      "   Accuracy: 0.9724\n",
      "   F1 Score: 0.9732\n",
      "\n",
      "NB Raw (Validation) Model Performance:\n",
      "   Accuracy: 0.9863\n",
      "   F1 Score: 0.9866\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score\n",
    "\n",
    "nb_raw = GaussianNB()\n",
    "\n",
    "nb_raw.fit(X_train, y_train)\n",
    "\n",
    "y_test_pred_raw = nb_raw.predict(X_test)\n",
    "y_val_pred_raw = nb_raw.predict(X_val)\n",
    "\n",
    "def evaluate_model(y_true, y_pred, model_type):\n",
    "    precision = precision_score(y_true, y_pred, average=\"binary\")\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    f1 = f1_score(y_true, y_pred)\n",
    "    print(f\"\\n{model_type} Model Performance:\")\n",
    "    print(f\"   Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"   F1 Score: {f1:.4f}\")\n",
    "    return accuracy, f1\n",
    "\n",
    "print(\"\\nEvaluating Na√Øve Bayes on Raw MFCC Features:\")\n",
    "test_results_raw = evaluate_model(y_test, y_test_pred_raw, \"NB Raw (Test)\")\n",
    "val_results_raw = evaluate_model(y_val, y_val_pred_raw, \"NB Raw (Validation)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, f1_score\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best n_estimators: 155\n",
      "Best random_state: 42\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "param_grid = {\n",
    "    'n_estimators': list(range(25, 201, 5)),\n",
    "    'random_state': [42],\n",
    "    'max_depth': [None]\n",
    "}\n",
    "\n",
    "rf = RandomForestClassifier()\n",
    "\n",
    "grid_search = GridSearchCV(rf, param_grid, cv=5, scoring='accuracy', n_jobs=-1)\n",
    "\n",
    "grid_search.fit(X_val, y_val)\n",
    "\n",
    "best_params = grid_search.best_params_\n",
    "print(\"Best n_estimators:\", best_params['n_estimators'])\n",
    "print(\"Best random_state:\", best_params['random_state'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating Base Values:\n",
      "\n",
      "rf Base (Test) Model Performance:\n",
      "Accuracy: 0.9804\n",
      "F1 Score: 0.9807\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.96      0.98      5600\n",
      "           1       0.96      1.00      0.98      5600\n",
      "\n",
      "    accuracy                           0.98     11200\n",
      "   macro avg       0.98      0.98      0.98     11200\n",
      "weighted avg       0.98      0.98      0.98     11200\n",
      "\n",
      "\n",
      "rf Base (Validation) Model Performance:\n",
      "Accuracy: 0.9960\n",
      "F1 Score: 0.9960\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.99      1.00      2200\n",
      "           1       0.99      1.00      1.00      2249\n",
      "\n",
      "    accuracy                           1.00      4449\n",
      "   macro avg       1.00      1.00      1.00      4449\n",
      "weighted avg       1.00      1.00      1.00      4449\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rf_base = RandomForestClassifier(n_estimators=best_params['n_estimators'], random_state=42)\n",
    "\n",
    "rf_base.fit(X_train, y_train)\n",
    "\n",
    "y_test_pred_base = rf_base.predict(X_test)\n",
    "y_val_pred_base = rf_base.predict(X_val)\n",
    "\n",
    "\n",
    "def evaluate_model(y_true, y_pred, model_type):\n",
    "    precision = precision_score(y_true, y_pred, average=\"binary\")\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    f1 = f1_score(y_true, y_pred)\n",
    "    print(f\"\\n{model_type} Model Performance:\")\n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"F1 Score: {f1:.4f}\")\n",
    "    print(f\"Classification Report:\\n{classification_report(y_true, y_pred)}\")\n",
    "    return precision, accuracy, f1\n",
    "\n",
    "print(\"\\nEvaluating Base Values:\")\n",
    "base_test_results = evaluate_model(y_test, y_test_pred_base, \"rf Base (Test)\")\n",
    "base_val_results = evaluate_model(y_val, y_val_pred_base, \"rf Base (Validation)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.9926\n",
      "Validation Precision: 0.9926\n",
      "Validation F1 Score: 0.9926\n",
      "Test Accuracy: 0.9258\n",
      "Test Precision: 0.9354\n",
      "Test F1 Score: 0.9254\n"
     ]
    }
   ],
   "source": [
    "gb_clf = GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, random_state=50)\n",
    "\n",
    "gb_clf.fit(X_train, y_train)\n",
    "\n",
    "y_val_pred = gb_clf.predict(X_val)\n",
    "\n",
    "y_test_pred = gb_clf.predict(X_test)\n",
    "\n",
    "val_accuracy = accuracy_score(y_val, y_val_pred)\n",
    "val_precision = precision_score(y_val, y_val_pred, average=\"weighted\")\n",
    "val_f1 = f1_score(y_val, y_val_pred, average=\"weighted\")\n",
    "\n",
    "print(f\"Validation Accuracy: {val_accuracy:.4f}\")\n",
    "print(f\"Validation Precision: {val_precision:.4f}\")\n",
    "print(f\"Validation F1 Score: {val_f1:.4f}\")\n",
    "\n",
    "test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "test_precision = precision_score(y_test, y_test_pred, average=\"weighted\")\n",
    "test_f1 = f1_score(y_test, y_test_pred, average=\"weighted\")\n",
    "\n",
    "print(f\"Test Accuracy: {test_accuracy:.4f}\")\n",
    "print(f\"Test Precision: {test_precision:.4f}\")\n",
    "print(f\"Test F1 Score: {test_f1:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
